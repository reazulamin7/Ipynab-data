{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "print(\"Tensorflow Version\",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a tf.data.Dataset\n",
    "data = pd.read_csv('training.1600000.processed.noemoticon.csv',encoding='latin', names = ['polarity','id','date','query','user','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1)\n",
    "data = data[:200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization \n",
    "\n",
    "# Dataset details target: the polarity of the tweet (0 = negative, 4 = positive)\n",
    "\n",
    "# date : the date of the tweet (Sat May 16 23:58:44 PDT 2009)\n",
    "# polarity : the polarity of the tweet (0 = negative 4 = positive)\n",
    "# user : the user that tweeted (TerraScene)\n",
    "# text : the text of the tweet (i'm 10x cooler than all of you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['polarity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the value 4 -->1 for ease of understanding.\n",
    "data['polarity'] = data['polarity'].replace(4,1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of positive vs. negative tagged sentences\n",
    "positives = data['polarity'][data.polarity == 1 ]\n",
    "negatives = data['polarity'][data.polarity == 0 ]\n",
    "\n",
    "print('Total length of the data is:         {}'.format(data.shape[0]))\n",
    "print('No. of positve tagged sentences is:  {}'.format(len(positives)))\n",
    "print('No. of negative tagged sentences is: {}'.format(len(negatives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a word count per of text\n",
    "def word_count(words):\n",
    "    return len(words.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot word count distribution for both positive and negative \n",
    "\n",
    "data['word count'] = data['text'].apply(word_count)\n",
    "p = data['word count'][data.polarity == 1]\n",
    "n = data['word count'][data.polarity == 0]\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.xlim(0,45)\n",
    "plt.xlabel('Word count')\n",
    "plt.ylabel('Frequency')\n",
    "g = plt.hist([p, n], color=['g','r'], alpha=0.5, label=['positive','negative'])\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get common words in training dataset\n",
    "from collections import Counter\n",
    "all_words = []\n",
    "for line in list(data['text']):\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "      if(len(word)>2):\n",
    "        all_words.append(word.lower())\n",
    "    \n",
    "    \n",
    "Counter(all_words).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Processing \n",
    "\n",
    "%matplotlib inline\n",
    "sns.countplot(data['polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the unnecessary columns.\n",
    "data.drop(['date','query','user','word count'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if any null values present\n",
    "(data.isnull().sum() / len(data))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convrting pandas object to a string type\n",
    "data['text'] = data['text'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopword = set(stopwords.words('english'))\n",
    "print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "userPattern = '@[^\\s]+'\n",
    "def process_tweets(tweet):\n",
    "  # Lower Casing\n",
    "    tweet = tweet.lower()\n",
    "    tweet=tweet[1:]\n",
    "    # Removing all URls \n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Removing all @username.\n",
    "    tweet = re.sub(userPattern,'', tweet) \n",
    "    #Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    #tokenizing words\n",
    "    tokens = word_tokenize(tweet)\n",
    "    #Removing Stop Words\n",
    "    final_tokens = [w for w in tokens if w not in stopword]\n",
    "    #reducing a word to its word stem \n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    for w in final_tokens:\n",
    "      if len(w)>1:\n",
    "        word = wordLemm.lemmatize(w)\n",
    "        finalwords.append(word)\n",
    "    return ' '.join(finalwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['processed_tweets'] = data['text'].apply(lambda x: process_tweets(x))\n",
    "print('Text Preprocessing complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the data \n",
    "\n",
    "Now we're going to analyse the preprocessed data to get an understanding of it. We'll plot Word Clouds for Positive and Negative tweets from our dataset and see which words occur the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-Cloud for Negative tweets.\n",
    "\n",
    "plt.figure(figsize = (15,15)) \n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.polarity == 0].processed_tweets))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-Cloud for Positive tweets\n",
    "\n",
    "plt.figure(figsize = (15,15)) \n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data[data.polarity == 1].processed_tweets))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorization and Splitting the data \n",
    "\n",
    "# vectoring input variable-processes_tweets to X and output variable-polarity to y\n",
    "\n",
    "X = data['processed_tweets'].values\n",
    "y = data['polarity'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to word frequency vectors\n",
    "\n",
    "#### TF-IDF\n",
    "\n",
    "- This is an acronym than stands for Term Frequency – Inverse Document Frequency which are the components of the resulting scores assigned to each word.\n",
    "\n",
    "- Term Frequency: This summarizes how often a given word appears within a document.\n",
    "\n",
    "- Inverse Document Frequency: This downscales words that appear a lot across documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "vector = TfidfVectorizer(sublinear_tf=True)\n",
    "X = vector.fit_transform(X)\n",
    "print(f'Vector fitted.')\n",
    "print('No. of feature_words: ', len(vector.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train and test\n",
    "\n",
    "The Preprocessed Data is divided into 2 sets of data:\n",
    "\n",
    "- Training Data: The dataset upon which the model would be trained on. Contains 80% data.\n",
    "\n",
    "- Test Data: The dataset upon which the model would be tested against. Contains 20% data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print()\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building \n",
    "\n",
    "### Model evaluating function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Evaluate(model):\n",
    "    #accuracy of model on training data\n",
    "    acc_train=model.score(X_train, y_train)\n",
    "    #accuracy of model on test data\n",
    "    acc_test=model.score(X_test, y_test)\n",
    "    \n",
    "    print('Accuracy of model on training data : {}'.format(acc_train*100))\n",
    "    print('Accuracy of model on testing data : {} \\n'.format(acc_test*100))\n",
    "\n",
    "    # Predict values for Test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    categories  = ['Negative','Positive']\n",
    "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Reds',fmt = '',\n",
    "                xticklabels = categories, yticklabels = categories)\n",
    "\n",
    "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression()\n",
    "history=lg.fit(X_train, y_train)\n",
    "model_Evaluate(lg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)\n",
    "model_Evaluate(svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 20, criterion = 'entropy', max_depth=50)\n",
    "rf.fit(X_train, y_train)\n",
    "model_Evaluate(rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = BernoulliNB()\n",
    "nb.fit(X_train, y_train)\n",
    "model_Evaluate(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN \n",
    "\n",
    "\n",
    "Recurrent neural networks (RNN) are the state of the art algorithm for sequential data and are used by Apple's Siri and and Google's voice search. It is the first algorithm that remembers its input, due to an internal memory, which makes it perfectly suited for machine learning problems that involve sequential data\n",
    "\n",
    "### Embedding Layer\n",
    "\n",
    "Embedding layer is one of the available layers in Keras. This is mainly used in Natural Language Processing related applications such as language modeling, but it can also be used with other tasks that involve neural networks. While dealing with NLP problems, we can use pre-trained word embeddings such as GloVe. Alternatively we can also train our own embeddings using Keras embedding layer.\n",
    "\n",
    "### LSTM layer\n",
    "\n",
    "Long Short Term Memory networks, usually called “LSTMs” , were introduced by Hochreiter and Schmiduber. These have widely been used for speech recognition, language modeling, sentiment analysis and text prediction. Before going deep into LSTM, we should first understand the need of LSTM which can be explained by the drawback of practical use of Recurrent Neural Network (RNN). So, lets start with RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers\n",
    "\n",
    "max_words = 5000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data.processed_tweets)\n",
    "sequences = tokenizer.texts_to_sequences(data.processed_tweets)\n",
    "tweets = pad_sequences(sequences, maxlen=max_len)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweets, data.polarity.values, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "model2 = Sequential()\n",
    "model2.add(layers.Embedding(max_words, 128))\n",
    "model2.add(layers.LSTM(64,dropout=0.5))\n",
    "model2.add(layers.Dense(16, activation='relu'))\n",
    "model2.add(layers.Dense(8, activation='relu'))\n",
    "model2.add(layers.Dense(1,activation='sigmoid'))\n",
    "model2.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "checkpoint2 = ModelCheckpoint(\"rnn_model.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "history = model2.fit(X_train, y_train, epochs=10,validation_data=(X_test, y_test),callbacks=[checkpoint2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['this data science article is the worst ever'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "pred = model2.predict(test)\n",
    "if pred > 0.5:\n",
    "  print('Positive')\n",
    "else:\n",
    "  print('Negative')\n",
    "# print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('rnn_model.hdf5')\n",
    "sequence = tokenizer.texts_to_sequences(['this data science article is the best ever'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "pred = model.predict(test)\n",
    "if pred > 0.5:\n",
    "  print('Positive')\n",
    "else:\n",
    "  print('Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences(['I had a bad day at work.'])\n",
    "test = pad_sequences(sequence, maxlen=max_len)\n",
    "pred = model.predict(test)\n",
    "if pred > 0.5:\n",
    "  print('Positive')\n",
    "else:\n",
    "  print('Negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving, Loading and Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file = open('vectoriser.pickle','wb')\n",
    "pickle.dump(vector, file)\n",
    "file.close()\n",
    "\n",
    "file = open('logisticRegression.pickle','wb')\n",
    "pickle.dump(lg, file)\n",
    "file.close()\n",
    "\n",
    "file = open('SVM.pickle','wb')\n",
    "pickle.dump(svm, file)\n",
    "file.close()\n",
    "\n",
    "file = open('RandomForest.pickle','wb')\n",
    "pickle.dump(rf, file)\n",
    "file.close()\n",
    "\n",
    "file = open('NaivesBayes.pickle','wb')\n",
    "pickle.dump(nb, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict using saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    # Load the vectoriser.\n",
    "    file = open('vectoriser.pickle', 'rb')\n",
    "    vectoriser = pickle.load(file)\n",
    "    file.close()\n",
    "    # Load the LR Model.\n",
    "    file = open('logisticRegression.pickle', 'rb')\n",
    "    lg = pickle.load(file)\n",
    "    file.close()\n",
    "    return vectoriser, lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(vectoriser, model, text):\n",
    "    # Predict the sentiment\n",
    "    processes_text=[process_tweets(sen) for sen in text]\n",
    "    textdata = vectoriser.transform(processes_text)\n",
    "    sentiment = model.predict(textdata)\n",
    "    \n",
    "    # Make a list of text with sentiment.\n",
    "    data = []\n",
    "    for text, pred in zip(text, sentiment):\n",
    "        data.append((text,pred))\n",
    "    # Convert the list into a Pandas DataFrame.\n",
    "    df = pd.DataFrame(data, columns = ['text','sentiment'])\n",
    "    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    # Loading the models.\n",
    "    vectoriser, lg = load_models()\n",
    "    \n",
    "    # Text to classify should be in a list.\n",
    "    text = [\"I love machine learning\",\n",
    "            \"Work is too hectic.\",\n",
    "            \"Mr.Sharama, I feel so good\"]\n",
    "    \n",
    "    df = predict(vectoriser, lg, text)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
